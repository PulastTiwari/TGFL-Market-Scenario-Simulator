{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da26cae6",
   "metadata": {},
   "source": [
    "# Baseline Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Data loading and preprocessing\n",
    "2. Central (non-federated) transformer training\n",
    "3. Scenario generation and evaluation\n",
    "4. Baseline metrics for comparison with federated learning\n",
    "\n",
    "**Hardware Requirements:** Optimized for CPU training on 8GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1224d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ML imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Project imports\n",
    "from data.loaders import MarketDataLoader, SyntheticDataGenerator\n",
    "from models.transformer import LightweightTransformer, create_small_transformer, create_tiny_transformer\n",
    "from evaluation.metrics import ScenarioEvaluator\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65728b79",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loaders\n",
    "data_loader = MarketDataLoader()\n",
    "synthetic_generator = SyntheticDataGenerator(seed=42)\n",
    "\n",
    "# Create synthetic training data (for demonstration without yfinance dependency)\n",
    "print(\"Generating synthetic training data...\")\n",
    "\n",
    "# Generate mixed regime data\n",
    "regime_lengths = {\n",
    "    'normal': 800,\n",
    "    'bull': 300, \n",
    "    'volatile': 200,\n",
    "    'bear': 400,\n",
    "    'normal': 300\n",
    "}\n",
    "\n",
    "train_data = synthetic_generator.create_mixed_regime_data(regime_lengths)\n",
    "print(f\"Generated {len(train_data)} training samples\")\n",
    "\n",
    "# Generate separate test data\n",
    "test_data = synthetic_generator.generate_regime_data('normal', length=500)\n",
    "print(f\"Generated {len(test_data)} test samples\")\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nTraining data summary:\")\n",
    "print(train_data[['close', 'returns', 'volatility']].describe())\n",
    "\n",
    "# Plot the synthetic data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price series\n",
    "axes[0, 0].plot(train_data.index, train_data['close'])\n",
    "axes[0, 0].set_title('Synthetic Price Series')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "\n",
    "# Returns\n",
    "axes[0, 1].plot(train_data.index, train_data['returns'])\n",
    "axes[0, 1].set_title('Daily Returns')\n",
    "axes[0, 1].set_ylabel('Returns')\n",
    "\n",
    "# Return distribution\n",
    "axes[1, 0].hist(train_data['returns'].dropna(), bins=50, alpha=0.7, density=True)\n",
    "axes[1, 0].set_title('Return Distribution')\n",
    "axes[1, 0].set_xlabel('Daily Returns')\n",
    "\n",
    "# Volatility\n",
    "axes[1, 1].plot(train_data.index, train_data['volatility'])\n",
    "axes[1, 1].set_title('Rolling Volatility')\n",
    "axes[1, 1].set_ylabel('Annualized Volatility')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719f76b",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b726f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training sequences\n",
    "sequence_length = 64  # Reduced for CPU training\n",
    "batch_size = 16       # Small batch size for 8GB RAM\n",
    "\n",
    "# Create sequences from training data\n",
    "print(\"Creating training sequences...\")\n",
    "train_sequences, train_targets = data_loader.create_sequences(\n",
    "    train_data, \n",
    "    sequence_length=sequence_length, \n",
    "    target_column='log_returns',\n",
    "    step_size=10  # Skip steps to reduce data size\n",
    ")\n",
    "\n",
    "print(f\"Created {len(train_sequences)} training sequences\")\n",
    "print(f\"Sequence shape: {train_sequences.shape}\")\n",
    "print(f\"Target shape: {train_targets.shape}\")\n",
    "\n",
    "# Create test sequences\n",
    "test_sequences, test_targets = data_loader.create_sequences(\n",
    "    test_data, \n",
    "    sequence_length=sequence_length,\n",
    "    target_column='log_returns',\n",
    "    step_size=5\n",
    ")\n",
    "\n",
    "print(f\"Created {len(test_sequences)} test sequences\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_X = torch.FloatTensor(train_sequences).unsqueeze(-1)  # Add feature dimension\n",
    "train_y = torch.FloatTensor(train_targets).unsqueeze(-1)\n",
    "test_X = torch.FloatTensor(test_sequences).unsqueeze(-1)\n",
    "test_y = torch.FloatTensor(test_targets).unsqueeze(-1)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "test_dataset = TensorDataset(test_X, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ab8f9",
   "metadata": {},
   "source": [
    "## 3. Create and Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301418cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tiny model for CPU training\n",
    "model = create_tiny_transformer()\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "\n",
    "# Define training configuration\n",
    "device = torch.device('cpu')  # Force CPU for consistency\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass - predict next value for each position\n",
    "        output = model(data)  # Shape: (batch_size, seq_len, 1)\n",
    "        \n",
    "        # Use last prediction as target\n",
    "        loss = criterion(output[:, -1, :], target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'  Batch {batch_idx:3d}/{len(train_loader)} | Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output[:, -1, :], target)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "print(\"Starting training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20  # Reduced for demo\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss = evaluate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    # Record losses\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.6f} | Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss\n",
    "        }, '../../data/models/baseline_best_model.pth')\n",
    "        print(f\"  → Best model saved (test loss: {test_loss:.6f})\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best test loss: {best_test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "plt.plot(epochs, test_losses, 'r-', label='Test Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Progress - Baseline Transformer')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final test loss: {test_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91442e8a",
   "metadata": {},
   "source": [
    "## 4. Generate Scenarios and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for generation\n",
    "checkpoint = torch.load('../../data/models/baseline_best_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Generate scenarios\n",
    "def generate_scenarios(model, num_scenarios=5, length=100, context_length=32):\n",
    "    \"\"\"Generate multiple market scenarios\"\"\"\n",
    "    scenarios = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_scenarios):\n",
    "            # Use random context from test data\n",
    "            context_idx = np.random.randint(0, len(test_X) - context_length)\n",
    "            context = test_X[context_idx:context_idx+1, :context_length, :]  # Shape: (1, context_len, 1)\n",
    "            \n",
    "            # Generate continuation\n",
    "            generated = model.generate(context, length=length, temperature=1.0)\n",
    "            \n",
    "            # Extract the generated part (remove context)\n",
    "            scenario = generated[0, context_length:, 0].numpy()  # Shape: (length,)\n",
    "            scenarios.append(scenario)\n",
    "            \n",
    "            if (i + 1) % 2 == 0:\n",
    "                print(f\"Generated scenario {i + 1}/{num_scenarios}\")\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "print(\"Generating scenarios...\")\n",
    "generated_scenarios = generate_scenarios(model, num_scenarios=5, length=200)\n",
    "print(f\"Generated {len(generated_scenarios)} scenarios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generated scenarios\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot historical returns for comparison\n",
    "historical_returns = test_data['log_returns'].dropna().values[-500:]\n",
    "plt.plot(historical_returns, alpha=0.7, color='black', linewidth=2, label='Historical (Test Data)')\n",
    "\n",
    "# Plot generated scenarios\n",
    "for i, scenario in enumerate(generated_scenarios):\n",
    "    plt.plot(scenario, alpha=0.7, linewidth=1.5, label=f'Generated {i+1}')\n",
    "\n",
    "plt.title('Generated Market Scenarios vs Historical Data')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Log Returns')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot return distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(historical_returns, bins=50, alpha=0.5, density=True, label='Historical', color='black')\n",
    "\n",
    "for i, scenario in enumerate(generated_scenarios):\n",
    "    plt.hist(scenario, bins=30, alpha=0.3, density=True, label=f'Generated {i+1}')\n",
    "\n",
    "plt.title('Return Distribution Comparison')\n",
    "plt.xlabel('Log Returns')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c51baa",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Statistical Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ScenarioEvaluator()\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"Running comprehensive evaluation...\")\n",
    "scenario_labels = [f'Baseline_Scenario_{i+1}' for i in range(len(generated_scenarios))]\n",
    "\n",
    "evaluation_results = evaluator.comprehensive_evaluation(\n",
    "    generated_scenarios, \n",
    "    historical_returns, \n",
    "    scenario_labels\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "evaluator.print_evaluation_summary(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of each metric\n",
    "print(\"\\nDETAILED EVALUATION RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for label, results in evaluation_results['scenarios'].items():\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  KS-test p-value: {results['ks_test']['p_value']:.4f}\")\n",
    "    print(f\"  ACF similarity: {results['acf_analysis']['acf_similarity_score']:.4f}\")\n",
    "    print(f\"  Volatility similarity: {results['volatility_analysis']['volatility_similarity_score']:.4f}\")\n",
    "    \n",
    "    moments = results['moments']\n",
    "    print(f\"  Mean return: {moments['mean']:.6f}\")\n",
    "    print(f\"  Volatility: {moments['std']:.6f}\")\n",
    "    print(f\"  Skewness: {moments['skewness']:.4f}\")\n",
    "    print(f\"  Kurtosis: {moments['kurtosis']:.4f}\")\n",
    "\n",
    "# Historical data statistics\n",
    "hist_moments = evaluation_results['historical']['moments']\n",
    "print(f\"\\nHistorical Data Statistics:\")\n",
    "print(f\"  Mean return: {hist_moments['mean']:.6f}\")\n",
    "print(f\"  Volatility: {hist_moments['std']:.6f}\")\n",
    "print(f\"  Skewness: {hist_moments['skewness']:.4f}\")\n",
    "print(f\"  Kurtosis: {hist_moments['kurtosis']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc877c72",
   "metadata": {},
   "source": [
    "## 6. Save Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9013bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare results for saving (convert numpy arrays to lists)\n",
    "def prepare_for_json(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: prepare_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [prepare_for_json(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "results_to_save = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_info': {\n",
    "        'type': 'baseline_transformer',\n",
    "        'parameters': model.count_parameters(),\n",
    "        'architecture': 'LightweightTransformer',\n",
    "        'sequence_length': sequence_length,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs_trained': num_epochs,\n",
    "        'best_test_loss': best_test_loss\n",
    "    },\n",
    "    'training_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'evaluation_results': prepare_for_json(evaluation_results),\n",
    "    'generated_scenarios': [scenario.tolist() for scenario in generated_scenarios]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "os.makedirs('../../data/results', exist_ok=True)\n",
    "with open('../../data/results/baseline_results.json', 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "print(\"Baseline results saved to: ../../data/results/baseline_results.json\")\n",
    "\n",
    "# Save scenarios as CSV for easy export\n",
    "scenarios_df = pd.DataFrame({\n",
    "    f'scenario_{i+1}': scenario \n",
    "    for i, scenario in enumerate(generated_scenarios)\n",
    "})\n",
    "scenarios_df.to_csv('../../data/results/baseline_scenarios.csv', index=False)\n",
    "print(\"Scenarios saved to: ../../data/results/baseline_scenarios.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c07c1",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"BASELINE MODEL TRAINING COMPLETED\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "summary = evaluation_results['summary']\n",
    "\n",
    "print(f\"Model Parameters: {model.count_parameters():,}\")\n",
    "print(f\"Training Epochs: {num_epochs}\")\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Best Test Loss: {best_test_loss:.6f}\")\n",
    "print(f\"\")\n",
    "print(f\"EVALUATION METRICS:\")\n",
    "print(f\"  Mean KS-test p-value: {summary['mean_ks_p_value']:.4f}\")\n",
    "print(f\"  Scenarios passing KS-test: {summary['pass_ks_test_005']}/{summary['num_scenarios']}\")\n",
    "print(f\"  Overall quality score: {summary['overall_quality_score']:.4f}\")\n",
    "\n",
    "# Check success criteria\n",
    "success = summary['mean_ks_p_value'] >= 0.05\n",
    "print(f\"\")\n",
    "if success:\n",
    "    print(\"✅ SUCCESS: Primary criterion met (KS p-value ≥ 0.05)\")\n",
    "else:\n",
    "    print(\"⚠️  PARTIAL: Primary criterion not fully met\")\n",
    "    print(\"   Consider: longer training, different architecture, or data preprocessing\")\n",
    "\n",
    "print(f\"\")\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Implement federated learning version\")\n",
    "print(\"2. Compare FL performance to this baseline\")\n",
    "print(\"3. Optimize hyperparameters if needed\")\n",
    "print(\"4. Create demo artifacts with these results\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
